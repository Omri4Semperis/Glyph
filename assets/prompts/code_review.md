# Your mission

Perform a code review of #{{what_is_being_reviewed}}

## Before starting

1. Read an example code review using `get_example("code_review")` to understand the expected format, structure, and how issues are tracked consistently throughout the document
2. Study the example carefully to see how issues progress from detailed sections to Bottom Line, Summary Table, Executive Summary, and Recommendations

## Additional References

- #{{design_log_name}}
- {{additional_context}} (if applicable)

## Review structure

The structure is design to offer the reader numerous ways to quickly understand the state of the code being reviewed- in varying levels of detail.

- Detailed sections - "Functionality", "Code/Inline documentation/Comments", "Testing/Coverage", "Documentation/Alignment of docs to code", "Performance & Security" (only when explicitly requested by the user)
- Lessons Learned
- The bottom line
- Summary Table - includes all detailed sections + lessons learned in a tabular format
- Executive summary (yes, towards the end)
- recommendations

In any point of the review, alongside the good things, an issue may be mentioned. For example, if a functionality is missing or code quality is poor. Mention these in the detailed sections, but also add them to the bottom line table and the summary table (Summary table gives an overview of the sections in a tabular format). In the doc itself, issues are indicated using special syntax (e.g. for an important issue, which is the third one found so far: `[issue_3 ‚ö†Ô∏è]`)

## CRITICAL: Issue Consistency Rules

**The Binary Principle**: Everything you find is EITHER:

- ‚úÖ Good/Pass - No action needed
- OR it's a tracked issue with severity (‚ùå Blocking, ‚ö†Ô∏è Important, ‚ÑπÔ∏è Nice-to-have, ü¶ã Cosmetic)

**There are NO standalone warnings, concerns, or recommendations.** If something is "partially handled", "could be improved", "consider doing X", or "nice to have" - it IS an issue and MUST be tracked as such.

**Issue Reference Format**: Always use `[issue_X emoji]` format where X is the issue number and emoji matches severity:

- `[issue_1 ‚ùå]` for blocking/critical
- `[issue_2 ‚ö†Ô∏è]` for important
- `[issue_3 ‚ÑπÔ∏è]` for nice-to-have
- `[issue_4 ü¶ã]` for cosmetic

**Every issue MUST appear in ALL of these places**:

1. **First mention** in detailed sections (where it's discovered)
2. **Bottom Line - Issues Summary table** (all issues listed)
3. **Summary Table** - Referenced in Status or Details column for relevant aspect (e.g., "‚ö†Ô∏è Warning [issue_1 ‚ö†Ô∏è, issue_2 ‚ö†Ô∏è]")
4. **Executive Summary** - Total count must match all tracked issues
5. **Recommendations section** - Every issue gets a recommendation

**Consistency Check**: Before finalizing the review, verify that:

- Each issue number appears consistently with the same severity emoji everywhere
- No warnings/concerns exist outside the issue tracking system
- The issue count in Executive Summary matches the Bottom Line table
- All issues from Bottom Line appear in Summary Table and Recommendations

**The structure**:

- Basic data:
  - Date
  - reviewer
  - what is being reviewed (Operation doc, Design log, etc.)
  - Other relevant references (PR link, commit hash, etc.)
- Detailed review
  - **IMPORTANT**: In EVERY detailed section below, use ONLY two types of markers:
    - ‚úÖ for things that are good/pass
    - `[issue_X emoji]` for ANYTHING that is not perfect (warnings, concerns, partial implementations, recommendations, nice-to-haves, etc.)
  - Functionality ("Do all implemented features match the requirements?")
    - Requirements Verification (A table with columns: Requirement, Status, Evidence)
    - Are edge cases handled? (Mark each as ‚úÖ or [issue_X emoji])
    - Deviations from design- Are they justified and documented?
  - Code Quality
    - Static code Analysis (number of lines in all reviewed modules + any abnormal findings that overpass a certain threshold, e.g., method with more than 20 lines)
    - Are SOLID, DRY, and KISS principles followed?
    - Does it follow project conventions and patterns?
    - Is the code clean, readable, and well-structured?
    - Are inline comments present for complex logic?
    - Is there appropriate error handling?
    - Logging
  - Testing
    - Unit tests (Table with 3 columns: Test file, tested file(s)/functionality, Test count, Coverage areas) - Are there sufficient unit tests?
    - Are integration tests present for key flows?
    - Are edge cases and error scenarios tested?
    - Testing quality and best practices (e.g., AAA pattern, use of mocks/stubs, etc.)
  - Documentation
    - Code and docs alignment
    - Any other relevant documentation (e.g., README, external docs, artifacts, operation document etc.)
    - Is external documentation updated? Should it be updated based on the review findings?
    - Are lessons learned documented appropriately?
  - Performance & Security (only when explicitly requested by the user)
    - Measured performance (e.g., response times, memory usage, etc.)
    - Are there any performance concerns?
    - Security review (e.g., input validation, authentication/authorization, data handling, etc.)
    - Are there security vulnerabilities?
  - Lessons Learned
    - What went well?
    - What could be improved?
    - Any unexpected findings or insights?
    - Review: Are all learned lessons documented in the appropriate documents (e.g., design log, operation document, etc.)? Beyond just the basic document we're reviewing.
- Executive summary- one paragraph with a Pass ‚úÖ or Fail ‚ùå for the review.
- Bottom line- An issues summary in the form of a table, with the following columns:
  - Issue # (1, 2, 3, etc.)
  - Category (Code/inline documentation, Testing, Documentation, Performance, Security, etc.)
  - Location(s) (As specific as possible, e.g., file name and method name, line numbers, etc.)
  - Origin (References that may have caused this issue, e.g., "dl_6 step 5" or "OP_123 Phase 2/Task 1")
  - Severity (Blocking/Critical ‚ùå, Important ‚ö†Ô∏è, Nice to have/Optional ‚ÑπÔ∏è, Cosmetic/Recommendation ü¶ã)
  - Recommendation (4 words max, e.g., "Refactor method X", "Add unit tests", "Update documentation", etc.)
- Summary Table with columns:
  - Aspect (Functionality, Code Quality, Architecture, Test Coverage, Documentation, Performance, Security, Design Log Compliance (if provided), Size Constraints)
  - Status (Pass ‚úÖ, Fail ‚ùå, Warning ‚ö†Ô∏è) - **MUST include issue references when status is Warning/Fail** (e.g., "‚ö†Ô∏è Warning [issue_1 ‚ö†Ô∏è, issue_2 ‚ö†Ô∏è]" or "‚ùå Blocking [issue_4 ‚ùå, issue_5 ‚ùå]")
  - Details (one short sentence per aspect, e.g., "All requirements met", "Clean code with minor issues", "Missing tests for edge cases", etc.)
- Executive summary- overall assessment. Must include:
  - Total count of ALL identified issues (must match Bottom Line table count)
  - Breakdown by severity (e.g., "2 Blocking, 5 Important, 1 Nice-to-have")
  - List each issue with its one-line summary
  - Final Pass ‚úÖ or Fail ‚ùå verdict
- Recommendations, which may take one of the following 3 forms:
  - "Nothing to add" (only if zero issues found)
  - "A few simple issues found, here are the recommended solutions/actions: <detailed recommendations for each issue by issue number>"
  - "Issues found (sorted here by severity). Here are basic recommendations: <brief recommendations for each issue by issue number>" And finish the section with a reminder that "A more detailed action plan should be created and addressed."
  - **CRITICAL**: Every issue from the Bottom Line table MUST have a corresponding recommendation listed here

## Final notes

- You may use Glyph's `create_code_review` tool to generate a template report based on the above structure (which it generates and saves in `.assistant/ad_hoc`), and fill it with the findings from your review.
- You may use Glyph's static code analysis tool to get some data about given files, and mermaid diagrams (through Glyph's mermaid tool) to visualize complex code structures or flows if needed.

**Before submitting the review, verify**:

1. Every issue number appears with consistent severity emoji throughout
2. No standalone issues/concerns exist without an issue tag
3. Issue count in Executive Summary = Row count in Bottom Line table
4. Every issue in Bottom Line appears in Summary Table (in relevant aspect rows)
5. Every issue in Bottom Line has a recommendation listed
